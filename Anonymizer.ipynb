{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8b8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f08776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc93a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "def resize_and_show(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    cv2.imshow(\"resizedimage\", image)\n",
    "    #waits for user to press any key \n",
    "    #(this is necessary to avoid Python kernel form crashing)\n",
    "    cv2.waitKey(0) \n",
    "    #closing all open windows \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d040ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af5afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0 720.0 30.00115834588208\n"
     ]
    }
   ],
   "source": [
    "#capture the video, and check video settings\n",
    "capture = cv2.VideoCapture('D://Research_Projects//TowardsMultimodalOpenScience//Video//sample.mp4')\n",
    "frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)   #fps = frames per second\n",
    "print(frameWidth, frameHeight, fps)\n",
    "#make an 'empty' video file where we project the poste tracking on\n",
    "samplerate = fps #make the same as current video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #(*'XVID')\n",
    "out = cv2.VideoWriter('AnonymizedVideos/'+'sample_anonymized'+'.mp4', fourcc, fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "# Run MediaPipe Holistic with `enable_segmentation=True` to get pose segmentation.\n",
    "with mp_holistic.Holistic(\n",
    "        static_image_mode=True, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "    while (True):\n",
    "        ret, image = capture.read() #read frames\n",
    "        if ret == True:\n",
    "            results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            # Draw pose segmentation.\n",
    "            annotated_image = image.copy()  \n",
    "            h, w, c = annotated_image.shape\n",
    "            original_image = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "            red_img = np.zeros_like(annotated_image, dtype=np.uint8)\n",
    "            red_img[:, :] = (255,255,255)\n",
    "            segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "            segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
    "            annotated_image = annotated_image * segm_2class * (1 - segm_2class)\n",
    "            annotated_image2 = annotated_image * segm_2class * (1 - segm_2class)\n",
    "                    # get the image dimensions (height, width and channels)\n",
    "            # append Alpha channel -- required for BGRA (Blue, Green, Red, Alpha)\n",
    "            mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "            # Zero background where we want to overlay\n",
    "            original_image[mask==0]=0\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                    original_image,\n",
    "                    results.face_landmarks,\n",
    "                    mp_holistic.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                    original_image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.\n",
    "                    get_default_pose_landmarks_style())\n",
    "            cv2.imshow(\"resizedimage\", original_image)#original_image)\n",
    "            out.write(original_image)\n",
    "            #waits for user to press any key \n",
    "            #(this is necessary to avoid Python kernel form crashing)\n",
    "            #cv2.waitKey(0)\n",
    "            if cv2.waitKey(1) == 27: #allow the use of ESCAPE to break the loop\n",
    "                break\n",
    "        if ret == False: #if there are no more frames, break the loop\n",
    "            break\n",
    "            \n",
    "#once done de-initialize\n",
    "out.release()\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820226a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  22%|█████████████▊                                                 | 84/382 [00:00<00:00, 782.23it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in .//Audio//sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "my_clip = mp.VideoFileClip(\".//Video//sample.mp4\")\n",
    "my_clip.audio.write_audiofile(\".//Audio//sample.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3aa28bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks\\chunk1.wav : Is the question i don't even get no to ask. \n",
      "audio-chunks\\chunk2.wav : The questionnaire will only arrive after you. \n",
      "audio-chunks\\chunk3.wav : Work your way to the edge and you find yourself standing on a new vista and staring out into the unknown and say i have a new question. \n",
      "audio-chunks\\chunk4.wav : About what's going on out there. \n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import pyttsx3 \n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened, language=\"en-EN\") # \"nl-NL\", \"es-ES\", \"fr-FR\", \"it-IT\"\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text\n",
    "    \n",
    "\n",
    "# Initialize the recognizer \n",
    "r = sr.Recognizer() \n",
    "\n",
    "speechtotext =  get_large_audio_transcription(\".//Audio//sample.wav\")\n",
    "#write transcriptions; note please check transcriptions to correct if neccesary\n",
    "with open('.//Transcriptions//sample.txt', 'w') as f:\n",
    "    f.write(speechtotext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e3ec89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'speech_recognition' has no attribute 'recognize_google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c09df6ffe44e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".//Audio//sample.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"es-ES\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'speech_recognition' has no attribute 'recognize_google'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f0923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################texttospeech\n",
    "from gtts import gTTS\n",
    "\n",
    "import os\n",
    "mytext = speechtotext \n",
    "\n",
    "language = 'en' #'nl', 'es', 'fr', 'it'\n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "myobj.save(\".//SynthesizedAudio//sample.wav\")\n",
    "os.system(\".//SynthesizedAudio//sample.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2f0b61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gtts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-53b538afe087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgtts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtts_langs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gtts' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e160b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Is the question i don't even get no to ask. The questionnaire will only arrive after you. Work your way to the edge and you find yourself standing on a new vista and staring out into the unknown and say i have a new question. About what's going on out there. \"]\n",
      "[0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input signal length=2 is too small to resample from 44100->11025",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m audio \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;241m1\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio)\n\u001b[1;32m---> 19\u001b[0m alignment \u001b[38;5;241m=\u001b[39m \u001b[43mpyfoal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\multimodalmask\\lib\\site-packages\\pyfoal\\core.py:49\u001b[0m, in \u001b[0;36malign\u001b[1;34m(text, audio, sample_rate)\u001b[0m\n\u001b[0;32m     46\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio) \u001b[38;5;241m/\u001b[39m sample_rate\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Resample\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Cache aligner\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(align, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maligner\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\.conda\\envs\\multimodalmask\\lib\\site-packages\\pyfoal\\core.py:296\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(audio, sample_rate)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"Resample audio\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m SAMPLE_RATE:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\.conda\\envs\\multimodalmask\\lib\\site-packages\\resampy\\core.py:97\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m shape[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(shape[axis] \u001b[38;5;241m*\u001b[39m sample_ratio)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput signal length=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is too small to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     98\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresample from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m.\u001b[39mshape[axis], sr_orig, sr_new))\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Preserve contiguity of input (if it exists)\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# If not, revert to C-contiguity by default\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF_CONTIGUOUS\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;31mValueError\u001b[0m: Input signal length=2 is too small to resample from 44100->11025"
     ]
    }
   ],
   "source": [
    "#force aligner\n",
    "from pydub.utils import mediainfo\n",
    "import numpy\n",
    "import pyfoal\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "\n",
    "text_file = \".//Transcriptions//sample.txt\"\n",
    "audio_file = \".//Audio//sample.wav\"\n",
    "sampling_rate, data=read_wav(audio_file) # enter your filename\n",
    "\n",
    "#read text\n",
    "with open(text_file) as f:\n",
    "    lines = f.readlines()\n",
    "print(lines)\n",
    "#load audio into 1D array\n",
    "audio = numpy.array(data,dtype=float)\n",
    "print(audio)\n",
    "\n",
    "alignment = pyfoal.align(text, audio, sample_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51a81408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100,\n",
       " array([[  0,   0],\n",
       "        [  0,   0],\n",
       "        [  0,   0],\n",
       "        ...,\n",
       "        [-13, -13],\n",
       "        [-14, -14],\n",
       "        [-16, -16]], dtype=int16))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d48f5a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4bf387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Engaging Warp Drive... *** \n",
      "\n",
      "Loading audio files...\n",
      " is not supported...\n",
      "Skipping _warpdrive_results...\n",
      "Files to process:\n",
      "- a_sample_original.wav\n",
      "- samplenew.wav\n",
      "\n",
      "For analysis only: resampling a_sample_original from 44100 to 24000 Hz...\n",
      "Using samplenew as base sig for comparison...\n",
      "Analyzing a_sample_original...\n",
      "Finding true base signal from results...\n",
      "Padding audio to align signals...\n",
      "Writing new audio files...\n",
      "Writing new audio files...\n",
      "Writing JSON metadata...\n",
      "Wahoo! That @#$%'s been successfully warped!'\n",
      "See results here: D:\\TowardsMultimodalOpenScience\\Audio\\TempToMerge\\_warpdrive_results\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b933c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85695d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
