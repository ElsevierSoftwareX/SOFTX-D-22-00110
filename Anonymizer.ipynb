{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8b8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f08776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc93a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "def resize_and_show(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    cv2.imshow(\"resizedimage\", image)\n",
    "    #waits for user to press any key \n",
    "    #(this is necessary to avoid Python kernel form crashing)\n",
    "    cv2.waitKey(0) \n",
    "    #closing all open windows \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d040ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af5afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0 720.0 30.00115834588208\n"
     ]
    }
   ],
   "source": [
    "#capture the video, and check video settings\n",
    "capture = cv2.VideoCapture('.//Video//sample.mp4')\n",
    "frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)   #fps = frames per second\n",
    "print(frameWidth, frameHeight, fps)\n",
    "#make an 'empty' video file where we project the poste tracking on\n",
    "samplerate = fps #make the same as current video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #(*'XVID')\n",
    "out = cv2.VideoWriter('AnonymizedVideos/'+'sample_anonymized'+'.mp4', fourcc, fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "# Run MediaPipe Holistic with `enable_segmentation=True` to get pose segmentation.\n",
    "with mp_holistic.Holistic(\n",
    "        static_image_mode=True, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "    while (True):\n",
    "        ret, image = capture.read() #read frames\n",
    "        if ret == True:\n",
    "            results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            # Draw pose segmentation.\n",
    "            annotated_image = image.copy()  \n",
    "            h, w, c = annotated_image.shape\n",
    "            original_image = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "            red_img = np.zeros_like(annotated_image, dtype=np.uint8)\n",
    "            red_img[:, :] = (255,255,255)\n",
    "            segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "            segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
    "            annotated_image = annotated_image * segm_2class * (1 - segm_2class)\n",
    "            annotated_image2 = annotated_image * segm_2class * (1 - segm_2class)\n",
    "                    # get the image dimensions (height, width and channels)\n",
    "            # append Alpha channel -- required for BGRA (Blue, Green, Red, Alpha)\n",
    "            mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "            # Zero background where we want to overlay\n",
    "            original_image[mask==0]=0\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                    original_image,\n",
    "                    results.face_landmarks,\n",
    "                    mp_holistic.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                    original_image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.\n",
    "                    get_default_pose_landmarks_style())\n",
    "            cv2.imshow(\"resizedimage\", original_image)#original_image)\n",
    "            out.write(original_image)\n",
    "            #waits for user to press any key \n",
    "            #(this is necessary to avoid Python kernel form crashing)\n",
    "            #cv2.waitKey(0)\n",
    "            if cv2.waitKey(1) == 27: #allow the use of ESCAPE to break the loop\n",
    "                break\n",
    "        if ret == False: #if there are no more frames, break the loop\n",
    "            break\n",
    "            \n",
    "#once done de-initialize\n",
    "out.release()\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820226a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in .//Audio//sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#extract audio from file and save it in Audio folder\n",
    "import moviepy.editor as mp\n",
    "my_clip = mp.VideoFileClip(\".//Video//sample.mp4\")\n",
    "my_clip.audio.write_audiofile(\".//Audio//sample.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3aa28bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydub.audio_segment.AudioSegment object at 0x0000022F8A110850>\n",
      "audio-chunks\\chunk1.wav : Is the question i don't even get no to ask. \n",
      "audio-chunks\\chunk2.wav : The questionnaire will only arrive after you. \n",
      "audio-chunks\\chunk3.wav : Work your way to the edge and you find yourself standing on a new vista and staring out into the unknown and say i have a new question. \n",
      "audio-chunks\\chunk4.wav : About what's going on out there. \n"
     ]
    }
   ],
   "source": [
    "#go through audio, chunk it, and extract text\n",
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import pyttsx3 \n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "\n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    \n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened, language=\"en-EN\") # \"nl-NL\", \"es-ES\", \"fr-FR\", \"it-IT\"\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text\n",
    "    \n",
    "\n",
    "# Initialize the recognizer \n",
    "r = sr.Recognizer() \n",
    "\n",
    "speechtotext =  get_large_audio_transcription(\".//Audio//sample.wav\")\n",
    "#write transcriptions; note please check transcriptions to correct if neccesary\n",
    "with open('.//Transcriptions//sample.txt', 'w') as f:\n",
    "    f.write(speechtotext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e3ec89",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautosub\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mautosub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;66;43;03m# pylint: disable=too-many-locals,too-many-arguments\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.//Audio//sample.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.//subtitles//sample.srt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdst_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubtitle_file_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import autosub\n",
    "\n",
    "autosub(# pylint: disable=too-many-locals,too-many-arguments\n",
    "        \".//Audio//sample.wav\",\n",
    "        output=\".//subtitles//sample.srt\",\n",
    "        src_language='en',\n",
    "        dst_language='en',\n",
    "        subtitle_file_format='srt',\n",
    "        api_key=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f0923f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mautosub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.//Audio//sample.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f0b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e160b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##force aligner throws error\n",
    "#from pydub.utils import mediainfo\n",
    "#import numpy\n",
    "#import pyfoal\n",
    "#from scipy.io.wavfile import read as read_wav\n",
    "#\n",
    "#text_file = \".//Transcriptions//sample.txt\"\n",
    "#audio_file = \".//Audio//sample.wav\"\n",
    "#sampling_rate, data=read_wav(audio_file) # enter your filename\n",
    "#\n",
    "##read text\n",
    "#with open(text_file) as f:\n",
    "#    lines = f.readlines()\n",
    "#print(lines)\n",
    "##load audio into 1D array\n",
    "#audio = numpy.array(data,dtype=float)\n",
    "#print(audio)\n",
    "#\n",
    "#alignment = pyfoal.align(text, audio, sample_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a81408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f5a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf387f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b933c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85695d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
